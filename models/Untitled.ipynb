{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QALSTMCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TextLineDataset([\"a\"])\n",
    "dataset.map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/angrypark/korean-text-matching-tf/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9f4f01c55187>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastText\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mJamoProcessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from gensim.models import FastText\n",
    "\n",
    "from utils.utils import JamoProcessor\n",
    "from models.base import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.apply_gradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(vocab_list_dir, \n",
    "                   pretrained_embed_dir, \n",
    "                   vocab_size, \n",
    "                   embed_dim):\n",
    "    embedding = np.random.uniform(-1/16, 1/16, [vocab_size, embed_dim])\n",
    "    if os.path.isfile(pretrained_embed_dir) & os.path.isfile(vocab_list_dir):\n",
    "        with open(vocab_list_dir, \"r\") as f:\n",
    "            vocab_list = [word.strip() for word in f if len(word)>0]\n",
    "        processor = JamoProcessor()\n",
    "        ft = FastText.load(pretrained_embed_dir)\n",
    "        num_oov = 0\n",
    "        for i, vocab in enumerate(vocab_list):\n",
    "            try:\n",
    "                embedding[i, :] = ft.wv[processor.word_to_jamo(vocab)]\n",
    "            except:\n",
    "                num_oov += 1\n",
    "        print(\"Pre-trained embedding loaded. Number of OOV : {} / {}\".format(num_oov, len(vocab_list)))\n",
    "    else:\n",
    "        print(\"No pre-trained embedding found, initialize with random distribution\")\n",
    "    return embedding\n",
    "\n",
    "def make_negative_mask(distances, num_negative_samples, method=\"random\"):\n",
    "    cur_batch_length = tf.shape(distances)[0]\n",
    "    if method == \"random\":\n",
    "        topk = tf.contrib.framework.sort(tf.nn.top_k(tf.random_uniform([cur_batch_length, cur_batch_length]), k=num_negative_samples).indices, axis=1)\n",
    "        rows = tf.transpose(tf.reshape(tf.tile(tf.range(cur_batch_length), [num_negative_samples]), [num_negative_samples, cur_batch_length]))\n",
    "        indices = tf.to_int64(tf.reshape(tf.concat([tf.expand_dims(rows, -1), tf.expand_dims(topk, -1)], axis=2), [num_negative_samples*cur_batch_length, 2]))\n",
    "        mask = tf.sparse_to_dense(sparse_indices=indices, \n",
    "                                  output_shape=[tf.to_int64(cur_batch_length), tf.to_int64(cur_batch_length)], \n",
    "                                  sparse_values=tf.ones([(num_negative_samples*cur_batch_length)], 1))\n",
    "        \n",
    "        # drop positive\n",
    "        mask = tf.multiply(mask, (1- tf.eye(cur_batch_length)))\n",
    "        \n",
    "    elif method == \"hard\":\n",
    "        topk = tf.contrib.framework.sort(tf.nn.top_k(distances, k=num_negative_samples+1).indices, axis=1)\n",
    "        rows = tf.transpose(tf.reshape(tf.tile(tf.range(cur_batch_length), [num_negative_samples+1]), [num_negative_samples+1, cur_batch_length]))\n",
    "        indices = tf.to_int64(tf.reshape(tf.concat([tf.expand_dims(rows, -1), tf.expand_dims(topk, -1)], axis=2), [(num_negative_samples+1)*cur_batch_length, 2]))\n",
    "        mask = tf.sparse_to_dense(sparse_indices=indices, \n",
    "                                  output_shape=[tf.to_int64(cur_batch_length), tf.to_int64(cur_batch_length)], \n",
    "                                  sparse_values=tf.ones([((num_negative_samples+1)*cur_batch_length)], 1))\n",
    "        # drop positive\n",
    "        mask = tf.multiply(mask, (1- tf.eye(cur_batch_length)))\n",
    "        \n",
    "    elif method == \"weighted\":\n",
    "        weight = tf.map_fn(lambda x: get_distance_weight(x, batch_size), tf.to_float(distances))\n",
    "        mask = weight\n",
    "#         mask = tf.to_int32(tf.contrib.framework.sort(tf.expand_dims(tf.multinomial(weight, num_negative_samples+1), -1), axis=1))\n",
    "#         weighted_samples_indices = tf.to_int32(tf.contrib.framework.sort(tf.expand_dims(tf.multinomial(weight, num_negative_samples+1), -1), axis=1))\n",
    "#         row_indices = tf.expand_dims(tf.transpose(tf.reshape(tf.tile(tf.range(0, batch_size, 1), [num_negative_samples+1]), [num_negative_samples+1, batch_size])), -1)\n",
    "#         mask_indices = tf.to_int64(tf.squeeze(tf.reshape(tf.concat([row_indices, weighted_samples_indices], 2), [(num_negative_samples+1)*batch_size,1,2])))\n",
    "#         mask_sparse = tf.SparseTensor(mask_indices, [1]*((num_negative_samples+1)*batch_size), [batch_size,batch_size])\n",
    "#         mask = tf.sparse_tensor_to_dense(mask_sparse)\n",
    "#         drop_positive = tf.to_int32(tf.subtract(tf.ones([batch_size, batch_size]), tf.eye(batch_size)))\n",
    "#         mask = tf.multiply(mask, drop_positive)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QALSTMCNN(BaseModel):\n",
    "    def __init__(self, data, config, mode=\"train\"):\n",
    "        super(QALSTMCNN, self).__init__(data, config)\n",
    "        self.mode = mode\n",
    "        self.build_model()\n",
    "        self.init_saver()\n",
    "        \n",
    "    def build_model(self):\n",
    "        # Build index table\n",
    "        index_table = tf.contrib.lookup.index_table_from_file(\n",
    "            vocabulary_file=self.config.vocab_list, \n",
    "            num_oov_buckets=0, \n",
    "            default_value=0)\n",
    "        \n",
    "        self.data_iterator = self.data.get_train_iterator(index_table) if self.mode==\"train\" else self.data.get_val_iterator(index_table)\n",
    "        \n",
    "        with tf.variable_scope(\"inputs\"):\n",
    "            # Placeholders for input, output\n",
    "            input_queries, input_replies, queries_lengths, replies_lengths = self.data_iterator.get_next()\n",
    "            self.input_queries = tf.placeholder_with_default(input_queries, [None, self.config.max_length], name=\"input_queries\")\n",
    "            self.input_replies = tf.placeholder_with_default(input_replies, [None, self.config.max_length], name=\"input_replies\")\n",
    "\n",
    "            self.queries_lengths = tf.placeholder_with_default(queries_lengths, [None], name=\"queries_length\")\n",
    "            self.replies_lengths = tf.placeholder_with_default(replies_lengths, [None], name=\"replies_length\")\n",
    "            \n",
    "            self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "            self.num_negative_samples = tf.placeholder(tf.int32, name=\"num_negative_samples\")\n",
    "        \n",
    "        cur_batch_length = tf.shape(self.input_queries)[0]\n",
    "        \n",
    "        # Define learning_rate and optimizer\n",
    "        learning_rate = tf.train.exponential_decay(self.config.learning_rate, \n",
    "                                                   self.global_step_tensor, \n",
    "                                                   decay_steps=50000, \n",
    "                                                   decay_rate=0.96, \n",
    "                                                   staircase=True)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        \n",
    "        # Embedding Layer\n",
    "        with tf.variable_scope(\"embedding\"):\n",
    "            embeddings = tf.Variable(get_embeddings(self.config.vocab_list, \n",
    "                                                    self.config.pretrained_embed_dir, \n",
    "                                                    self.config.vocab_size, \n",
    "                                                    self.config.embed_dim),\n",
    "                                     trainable=True, \n",
    "                                     name=\"embeddings\")\n",
    "            queries_embedded = tf.nn.embedding_lookup(embeddings, self.input_queries, name=\"queries_embedded\")\n",
    "            replies_embedded = tf.nn.embedding_lookup(embeddings, self.input_replies, name=\"replies_embedded\")\n",
    "            queries_embedded, replies_embedded = tf.cast(queries_embedded, tf.float32), tf.cast(replies_embedded, tf.float32)\n",
    "\n",
    "        # Build LSTM Layer\n",
    "        query_lstm_cell_fw = tf.nn.rnn_cell.LSTMCell(self.config.lstm_dim, \n",
    "                                                     use_peepholes=True,\n",
    "                                                     name=\"query_fw\")\n",
    "        query_lstm_cell_bw = tf.nn.rnn_cell.LSTMCell(self.config.lstm_dim, \n",
    "                                                     use_peepholes=True,\n",
    "                                                     name=\"query_bw\")\n",
    "        query_lstm_cell_fw = tf.nn.rnn_cell.LSTMCell(self.config.lstm_dim, \n",
    "                                                     use_peepholes=True,\n",
    "                                                     name=\"reply_fw\")\n",
    "        query_lstm_cell_bw = tf.nn.rnn_cell.LSTMCell(self.config.lstm_dim, \n",
    "                                                     use_peepholes=True,\n",
    "                                                     name=\"reply_bw\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.rnn_cell.LSTMCell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.dynamic_rnn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "angryenv",
   "language": "python",
   "name": "angryenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
