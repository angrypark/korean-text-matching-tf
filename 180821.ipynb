{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 180821\n",
    "### Model 관련\n",
    "- validation에 대해 eval metric 구현 : MAP, nDCG, f1-score, AUROC\n",
    "- prototyping page 구현\n",
    "- TCN 모델 넣기\n",
    "- weighted sampling 구현\n",
    "\n",
    "### 기획 관련\n",
    "- 테스트셋 수정\n",
    "- Reply set 작성법 만들기\n",
    "---\n",
    "## Tensorflow로 eval metric 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "lstm_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_queries = np.random.random(size=[batch_size, lstm_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_weight(distance, n, cutoff=0):\n",
    "    log_weights = (2.0-float(n))*tf.log(distance) - (float(n-3)/2)*tf.log(1.0-0.25*(distance**2.0))\n",
    "    weight = tf.exp(log_weights)\n",
    "    is_na = tf.is_nan(weight)\n",
    "    weight = tf.where(tf.is_nan(weight), tf.to_float(tf.fill(weight.shape, cutoff)), weight)\n",
    "    return tf.minimum(weight, tf.to_float(tf.fill(tf.shape(weight), cutoff)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"cpu:0\"):\n",
    "    encoding_queries = tf.constant(np.random.random(size=[batch_size, lstm_dim]))\n",
    "    encoding_replies = tf.constant(np.random.random(size=[batch_size, lstm_dim]))\n",
    "    distance = tf.matmul(encoding_queries, encoding_replies, transpose_b=True)\n",
    "    log_weights = (2.0-float(batch_size))*tf.log(distance) - (float(batch_size-3)/2)*tf.log(1.0-0.25*(distance**2.0))\n",
    "    weight = tf.exp(log_weights)\n",
    "    is_na = tf.is_nan(weight)\n",
    "    \n",
    "    # weight = tf.map_fn(lambda x: get_distance_weight(x, batch_size), tf.to_float(distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_config = tf.ConfigProto(log_device_placement=True)\n",
    "sess = tf.Session(config=tf_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       ...,\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(is_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006127097573297671"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e-2*(0.96**12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t25433531\n",
      "1\t22438994\n",
      "2\t28275733\n",
      "3\t30539388\n",
      "4\t23822359\n",
      "5\t22571455\n",
      "6\t21582304\n",
      "7\t23153158\n",
      "8\t29596797\n",
      "9\t22033600\n",
      "10\t24918468\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/media/scatter/scatterdisk/reply_matching_model/sol.tokenized.sent_piece_50K/sol.tokenized_12.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-0bccac9d4ca7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtotal_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}\\t{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/scatter/scatterdisk/reply_matching_model/sol.tokenized.sent_piece_50K/sol.tokenized_12.txt'"
     ]
    }
   ],
   "source": [
    "base_dir = \"/media/scatter/scatterdisk/reply_matching_model/sol.tokenized.sent_piece_50K/sol.tokenized_{}.txt\"\n",
    "total_length = 0\n",
    "for i in range(12):\n",
    "    with open(base_dir.format(i+1), \"r\") as f:\n",
    "        length = sum([1 for line in f])\n",
    "        print(\"{}\\t{}\".format(i, length))\n",
    "        total_length += length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\t24189168\n"
     ]
    }
   ],
   "source": [
    "with open(base_dir.format(12), \"r\") as f:\n",
    "    length = sum([1 for line in f])\n",
    "    print(\"{}\\t{}\".format(12, length))\n",
    "    total_length += length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from models.base import BaseModel\n",
    "from models.tcn_ops import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(idx2word, config):\n",
    "    embedding = np.random.uniform(-1/16, 1/16, [config.vocab_size, config.embed_dim])\n",
    "    if config.pretrained_embed_dir:\n",
    "        processor = JamoProcessor()\n",
    "        ft = FastText.load(config.pretrained_embed_dir)\n",
    "        num_oov = 0\n",
    "        for i, vocab in enumerate(idx2word):\n",
    "            try:\n",
    "                embedding[i, :] = ft.wv[processor.word_to_jamo(vocab)]\n",
    "            except:\n",
    "                num_oov += 1\n",
    "        print(\"Pre-trained embedding loaded. Number of OOV : {} / {}\".format(num_oov, len(idx2word)))\n",
    "    else:\n",
    "        print(\"No pre-trained embedding found, initialize with random distribution\")\n",
    "    return embedding\n",
    "\n",
    "def make_negative_mask(distances, method=\"random\", num_negative_samples=2, batch_size=256):\n",
    "    if method == \"random\":\n",
    "        mask = np.zeros([batch_size, batch_size])\n",
    "        for i in range(batch_size):\n",
    "            indices = np.random.choice([j for j in range(batch_size) if j != i], size=num_negative_samples, replace=False)\n",
    "            mask[i, indices] = True\n",
    "            mask[i, i] = False\n",
    "        mask = tf.convert_to_tensor(mask)\n",
    "    elif method == \"hard\":\n",
    "        top_k = tf.contrib.framework.sort(tf.expand_dims(tf.nn.top_k(-distances, k=num_negative_samples+1).indices, -1), axis=1)\n",
    "        row_indices = tf.expand_dims(tf.transpose(tf.reshape(tf.tile(tf.range(0, batch_size, 1), [num_negative_samples+1]), [num_negative_samples+1, batch_size])), -1)\n",
    "        mask_indices = tf.to_int64(tf.squeeze(tf.reshape(tf.concat([row_indices, top_k], 2), [(num_negative_samples+1)*batch_size,1,2])))\n",
    "        mask_sparse = tf.SparseTensor(mask_indices, [1]*((num_negative_samples+1)*batch_size), [batch_size,batch_size])\n",
    "        mask = tf.sparse_tensor_to_dense(mask_sparse)\n",
    "        drop_positive = tf.to_int32(tf.subtract(tf.ones([batch_size, batch_size]), tf.eye(batch_size)))\n",
    "        mask = tf.multiply(mask, drop_positive)\n",
    "    elif method == \"weighted\":\n",
    "        weight = tf.map_fn(lambda x: get_distance_weight(x, batch_size), tf.to_float(distances))\n",
    "        mask = weight\n",
    "#         mask = tf.to_int32(tf.contrib.framework.sort(tf.expand_dims(tf.multinomial(weight, num_negative_samples+1), -1), axis=1))\n",
    "#         weighted_samples_indices = tf.to_int32(tf.contrib.framework.sort(tf.expand_dims(tf.multinomial(weight, num_negative_samples+1), -1), axis=1))\n",
    "#         row_indices = tf.expand_dims(tf.transpose(tf.reshape(tf.tile(tf.range(0, batch_size, 1), [num_negative_samples+1]), [num_negative_samples+1, batch_size])), -1)\n",
    "#         mask_indices = tf.to_int64(tf.squeeze(tf.reshape(tf.concat([row_indices, weighted_samples_indices], 2), [(num_negative_samples+1)*batch_size,1,2])))\n",
    "#         mask_sparse = tf.SparseTensor(mask_indices, [1]*((num_negative_samples+1)*batch_size), [batch_size,batch_size])\n",
    "#         mask = tf.sparse_tensor_to_dense(mask_sparse)\n",
    "#         drop_positive = tf.to_int32(tf.subtract(tf.ones([batch_size, batch_size]), tf.eye(batch_size)))\n",
    "#         mask = tf.multiply(mask, drop_positive)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_padding(x, padding=(1, 1)):\n",
    "    \"\"\"Pads the middle dimension of a 3D tensor.\n",
    "    # Arguments\n",
    "        x: Tensor or variable.\n",
    "        padding: Tuple of 2 integers, how many zeros to\n",
    "            add at the start and end of dim 1.\n",
    "    # Returns\n",
    "        A padded 3D tensor.\n",
    "    \"\"\"\n",
    "    assert len(padding) == 2\n",
    "    pattern = [[0, 0], [padding[0], padding[1]], [0, 0]]\n",
    "    return tf.pad(x, pattern)\n",
    "\n",
    "def attentionBlock(x):\n",
    "    \"\"\"self attention block\n",
    "    # Arguments\n",
    "        x: Tensor of shape [N, L, Cin]\n",
    "    \"\"\"\n",
    "\n",
    "    k_size = x.get_shape()[-1].value\n",
    "    v_size = x.get_shape()[-1].value\n",
    "\n",
    "    key = tf.layers.dense(x, units=k_size, activation=None, use_bias=False, kernel_initializer=tf.random_normal_initializer(0, 0.01)) # [N, L, k_size]\n",
    "    #query = tf.layers.dense(x, units=k_size, activation=None, use_bias=False, kernel_initializer=tf.random_normal_initializer(0, 0.01)) # [N, L, k_size]\n",
    "    value = tf.layers.dense(x, units=v_size, activation=None, use_bias=False, kernel_initializer=tf.random_normal_initializer(0, 0.01))\n",
    "    \n",
    "    logits = tf.matmul(key, key, transpose_b=True)\n",
    "    logits = logits / np.sqrt(k_size)\n",
    "    weights = tf.nn.softmax(logits, name=\"attention_weights\") # N, L, ksize\n",
    "    output = tf.matmul(weights, value)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_arg_scope\n",
    "def weightNormConvolution1d(x, num_filters, dilation_rate, filter_size=3, stride=[1],\n",
    "                            pad='VALID', init_scale=1., init=False, gated=False,\n",
    "                            counters={}):\n",
    "    name = get_name('weightnorm_conv1d', counters)\n",
    "    with tf.variable_scope(name):\n",
    "        # currently this part is never used\n",
    "        if init:\n",
    "            print(\"initializing weight norm\")\n",
    "            # data based initialization of parameters\n",
    "            V = tf.get_variable('V', [filter_size, int(x.get_shape()[-1]), num_filters],\n",
    "                                tf.float32, tf.random_normal_initializer(0, 0.01),\n",
    "                                trainable=True)\n",
    "            V_norm = tf.nn.l2_normalize(V.initialized_value(), [0, 1])\n",
    "\n",
    "            # pad x\n",
    "            left_pad = dilation_rate * (filter_size - 1)\n",
    "            x = temporal_padding(x, (left_pad, 0))\n",
    "            x_init = tf.nn.convolution(x, V_norm, pad, stride, [dilation_rate])\n",
    "            #x_init = tf.nn.conv2d(x, V_norm, [1]+stride+[1], pad)\n",
    "            m_init, v_init = tf.nn.moments(x_init, [0, 1])\n",
    "            scale_init = init_scale/tf.sqrt(v_init + 1e-8)\n",
    "            g = tf.get_variable('g', dtype=tf.float32, initializer=scale_init,\n",
    "                                trainable=True)\n",
    "            b = tf.get_variable('b', dtype=tf.float32, initializer=-m_init*scale_init,\n",
    "                                trainable=True)\n",
    "            x_init = tf.reshape(scale_init, [1, 1, num_filters]) \\\n",
    "                                * (x_init - tf.reshape(m_init, [1, 1, num_filters]))\n",
    "            # apply nonlinearity\n",
    "            x_init = tf.nn.relu(x_init)\n",
    "            return x_init\n",
    "\n",
    "        else:\n",
    "            # Gating mechanism (Dauphin 2016 LM with Gated Conv. Nets)\n",
    "            if gated:\n",
    "                num_filters = num_filters * 2\n",
    "\n",
    "            # size of V is L, Cin, Cout\n",
    "            V = tf.get_variable('V', [filter_size, int(x.get_shape()[-1]), num_filters],\n",
    "                                tf.float32, initializer=None,\n",
    "                                trainable=True)\n",
    "            g = tf.get_variable('g', shape=[num_filters], dtype=tf.float32,\n",
    "                                initializer=tf.constant_initializer(1.), trainable=True)\n",
    "            b = tf.get_variable('b', shape=[num_filters], dtype=tf.float32,\n",
    "                                initializer=None, trainable=True)\n",
    "\n",
    "            # size of input x is N, L, Cin\n",
    "\n",
    "            # use weight normalization (Salimans & Kingma, 2016)\n",
    "            W = tf.reshape(g, [1, 1, num_filters]) * tf.nn.l2_normalize(V, [0, 1])\n",
    "\n",
    "            # pad x for causal convolution\n",
    "            left_pad = dilation_rate * (filter_size  - 1)\n",
    "            x = temporal_padding(x, (left_pad, 0))\n",
    "\n",
    "            # calculate convolutional layer output\n",
    "            x = tf.nn.bias_add(tf.nn.convolution(x, W, pad, stride, [dilation_rate]), b)\n",
    "\n",
    "            # GLU\n",
    "            if gated:\n",
    "                split0, split1 = tf.split(x, num_or_size_splits=2, axis=2)\n",
    "                split1 = tf.sigmoid(split1)\n",
    "                x = tf.multiply(split0, split1)\n",
    "            # ReLU\n",
    "            else:\n",
    "                # apply nonlinearity\n",
    "                x = tf.nn.relu(x)\n",
    "\n",
    "            print(x.get_shape())\n",
    "\n",
    "            return x\n",
    "\n",
    "def TemporalBlock(input_layer, out_channels, filter_size, stride, dilation_rate, counters,\n",
    "                  dropout, init=False, atten=False, use_highway=False, gated=False):\n",
    "\n",
    "    keep_prob = 1.0 - dropout\n",
    "\n",
    "    in_channels = input_layer.get_shape()[-1]\n",
    "    name = get_name('temporal_block', counters)\n",
    "    with tf.variable_scope(name):\n",
    "\n",
    "        # num_filters is the hidden units in TCN\n",
    "        # which is the number of out channels\n",
    "        conv1 = weightNormConvolution1d(input_layer, out_channels, dilation_rate,\n",
    "                                        filter_size, [stride], counters=counters,\n",
    "                                        init=init, gated=gated)\n",
    "        # set noise shape for spatial dropout\n",
    "        # refer to https://colab.research.google.com/drive/1la33lW7FQV1RicpfzyLq9H0SH1VSD4LE#scrollTo=TcFQu3F0y-fy\n",
    "        # shape should be [N, 1, C]\n",
    "        noise_shape = (tf.shape(conv1)[0], tf.constant(1), tf.shape(conv1)[2])\n",
    "        dropout1 = tf.nn.dropout(conv1, keep_prob, noise_shape)\n",
    "        if atten:\n",
    "            dropout1 = attentionBlock(dropout1)\n",
    "\n",
    "        conv2 = weightNormConvolution1d(dropout1, out_channels, dilation_rate, filter_size,\n",
    "            [stride], counters=counters, init=init, gated=gated)\n",
    "        dropout2 = tf.nn.dropout(conv2, keep_prob, noise_shape)\n",
    "        if atten:\n",
    "            dropout2 = attentionBlock(dropout2)\n",
    "\n",
    "        # highway connetions or residual connection\n",
    "        residual = None\n",
    "        if use_highway:\n",
    "            W_h = tf.get_variable('W_h', [1, int(input_layer.get_shape()[-1]), out_channels],\n",
    "                                  tf.float32, tf.random_normal_initializer(0, 0.01), trainable=True)\n",
    "            b_h = tf.get_variable('b_h', shape=[out_channels], dtype=tf.float32,\n",
    "                                  initializer=None, trainable=True)\n",
    "            H = tf.nn.bias_add(tf.nn.convolution(input_layer, W_h, 'SAME'), b_h)\n",
    "\n",
    "            W_t = tf.get_variable('W_t', [1, int(input_layer.get_shape()[-1]), out_channels],\n",
    "                                  tf.float32, tf.random_normal_initializer(0, 0.01), trainable=True)\n",
    "            b_t = tf.get_variable('b_t', shape=[out_channels], dtype=tf.float32,\n",
    "                                  initializer=None, trainable=True)\n",
    "            T = tf.nn.bias_add(tf.nn.convolution(input_layer, W_t, 'SAME'), b_t)\n",
    "            T = tf.nn.sigmoid(T)\n",
    "            residual = H*T + input_layer * (1.0 - T)\n",
    "        elif in_channels != out_channels:\n",
    "            W_h = tf.get_variable('W_h', [1, int(input_layer.get_shape()[-1]), out_channels],\n",
    "                                  tf.float32, tf.random_normal_initializer(0, 0.01), trainable=True)\n",
    "            b_h = tf.get_variable('b_h', shape=[out_channels], dtype=tf.float32,\n",
    "                                  initializer=None, trainable=True)\n",
    "            residual = tf.nn.bias_add(tf.nn.convolution(input_layer, W_h, 'SAME'), b_h)\n",
    "        else:\n",
    "            print(\"no residual convolution\")\n",
    "\n",
    "        res = input_layer if residual is None else residual\n",
    "\n",
    "        return tf.nn.relu(dropout2 + res)\n",
    "\n",
    "def TemporalConvNet(input_layer, num_channels, sequence_length, kernel_size=2,\n",
    "                    dropout=tf.constant(0.0, dtype=tf.float32), init=False,\n",
    "                    atten=False, use_highway=False, use_gated=False):\n",
    "    num_levels = len(num_channels)\n",
    "    counters = {}\n",
    "    for i in range(num_levels):\n",
    "        print(i)\n",
    "        dilation_size = 2 ** i\n",
    "        out_channels = num_channels[i]\n",
    "        input_layer = TemporalBlock(input_layer, out_channels, kernel_size, stride=1, dilation_rate=dilation_size,\n",
    "                                 counters=counters, dropout=dropout, init=init, atten=atten, gated=use_gated)\n",
    "\n",
    "    return input_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualEncoderTCN(BaseModel):\n",
    "    def __init__(self, preprocessor, config):\n",
    "        super(TCN, self).__init__(preprocessor, config)\n",
    "        self.build_model()\n",
    "        self.init_saver()\n",
    "        \n",
    "    def build_model(self):\n",
    "        with tf.variable_scope(\"inputs\"):\n",
    "            # Placeholders for input, output\n",
    "            self.input_queries = tf.placeholder(tf.int32, [None, self.config.max_length], name=\"input_queries\")\n",
    "            self.input_replies = tf.placeholder(tf.int32, [None, self.config.max_length], name=\"input_replies\")\n",
    "\n",
    "            self.queries_lengths = tf.placeholder(tf.int32, [None], name=\"queries_length\")\n",
    "            self.replies_lengths = tf.placeholder(tf.int32, [None], name=\"replies_length\")\n",
    "            self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        cur_batch_length = tf.shape(self.input_queries)[0]\n",
    "        \n",
    "        # Define learning rate and optimizer\n",
    "        learning_rate = tf.train.exponential_decay(self.config.learning_rate, \n",
    "                                                   self.global_step_tensor,\n",
    "                                                   decay_steps=50000, \n",
    "                                                   decay_rate=0.96,\n",
    "                                                   staircase=True)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        \n",
    "        # Embedding layer\n",
    "        with tf.variable_scope(\"embedding\"):\n",
    "            embeddings = tf.Variable(get_embeddings(self.preprocessor.vectorizer.idx2word, self.config), trainable=True, name=\"embeddings\")\n",
    "            queries_embedded = tf.nn.embedding_lookup(embeddings, self.input_queries, name=\"queries_embedded\")\n",
    "            replies_embedded = tf.nn.embedding_lookup(embeddings, self.input_replies, name=\"replies_embedded\")\n",
    "            queries_embedded, replies_embedded = tf.cast(queries_embedded, tf.float32), tf.cast(replies_embedded, tf.float32)\n",
    "        \n",
    "        # Dropout\n",
    "        queries_embedded = tf.nn.dropout(queries_embedded, keep_prob=self.config.dropout_keep_prob)\n",
    "        replies_embedded = tf.nn.dropout(replies_embedded, keep_prob=self.config.dropout_keep_prob)\n",
    "        \n",
    "        # Use TCN same as rnn cell\n",
    "        encoding_queries = TemporalConvNet(input_layer=queries_embedded, \n",
    "                                         num_channels=self.config.tcn_num_channels, \n",
    "                                         sequence_length = self.queries_lengths, \n",
    "                                         kernel_size=self.config.tcn_kernel_size, \n",
    "                                         dropout=1-self.config.dropout_keep_prob, \n",
    "                                         init=False)\n",
    "        encoding_replies = TemporalConvNet(input_layer=replies_embedded, \n",
    "                                         num_channels=self.config.tcn_num_channels, \n",
    "                                         sequence_length = self.queries_lengths, \n",
    "                                         kernel_size=self.config.tcn_kernel_size, \n",
    "                                         dropout=1-self.config.dropout_keep_prob, \n",
    "                                         init=False)\n",
    "        \n",
    "        # Predict a response\n",
    "        with tf.variable_scope(\"prediction\") as vs:\n",
    "            M = tf.get_variable(\"M\",\n",
    "                                shape=[self.config.embedding_dim, self.config.embedding_dim],\n",
    "                                initializer=tf.truncated_normal_initializer())\n",
    "            encoding_queries = tf.matmul(encoding_queries, M)\n",
    "            \n",
    "        with tf.variable_scope(\"negative_sampling\") as vs:\n",
    "            distances = tf.matmul(encoding_queries, tf.transpose(encoding_replies))\n",
    "            positive_mask = tf.reshape(tf.eye(cur_batch_length), [-1])\n",
    "            negative_mask = make_negative_mask(distances,\n",
    "                                               method=self.config.negative_sampling,\n",
    "                                               num_negative_samples=self.config.num_negative_samples,\n",
    "                                               batch_size=self.config.batch_size)\n",
    "            \n",
    "            # slice negative mask for when current batch size is smaller than predefined batch size\n",
    "            negative_mask = tf.slice(negative_mask, [0,0], [cur_batch_length, cur_batch_length])\n",
    "            negative_mask = tf.reshape(negative_mask, [-1])\n",
    "        \n",
    "        with tf.variable_scope(\"logits\"):\n",
    "            positive_logits = tf.gather(tf.reshape(distances, [-1]), tf.where(positive_mask), 1)\n",
    "            self.positive_probs = tf.sigmoid(positive_logits)\n",
    "            negative_logits = tf.gather(tf.reshape(distances, [-1]), tf.where(negative_mask), 1)\n",
    "            num_positives = tf.shape(positive_logits)[0]\n",
    "            num_negatives = tf.shape(negative_logits)[0]\n",
    "            self.logits = tf.concat([positive_logits, negative_logits], 0)\n",
    "\n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.variable_scope(\"loss\"):\n",
    "            self.labels = tf.to_float(tf.concat([tf.ones([num_positives, 1]), tf.zeros([num_negatives, 1])], 0))\n",
    "            losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=self.labels)\n",
    "            self.losses = losses # DEBUG\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "            self.train_step = self.optimizer.minimize(self.loss, global_step=self.global_step_tensor)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        with tf.name_scope(\"score\"):\n",
    "            # Apply sigmoid to convert logits to probabilities\n",
    "            self.probs = tf.sigmoid(self.logits)\n",
    "            self.predictions = tf.cast(self.probs > 0.5, dtype=tf.int32)\n",
    "            correct_predictions = tf.equal(self.predictions, tf.to_int32(self.labels))\n",
    "            self.score = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "angryenv",
   "language": "python",
   "name": "angryenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
