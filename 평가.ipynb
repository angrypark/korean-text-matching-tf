{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이름만 바꿔서 실행하면 됩니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "from data_loader import DataGenerator\n",
    "\n",
    "from trainer import MatchingModelTrainer\n",
    "from preprocessor import DynamicPreprocessor\n",
    "from utils.dirs import create_dirs\n",
    "from utils.logger import SummaryWriter\n",
    "from utils.config import load_config, save_config\n",
    "from models.base import get_model\n",
    "from utils.utils import JamoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"soynlp_ns4_lr1e-3\"\n",
    "TOKENIZER = \"SoyNLPTokenizer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/media/scatter/scatterdisk/reply_matching_model/runs/{}/\".format(NAME)\n",
    "config_dir = base_dir + \"config.json\"\n",
    "best_model_dir = base_dir + \"best_loss/best_loss.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = load_config(config_dir)\n",
    "preprocessor = DynamicPreprocessor(model_config)\n",
    "preprocessor.build_preprocessor()\n",
    "\n",
    "infer_config = load_config(config_dir)\n",
    "setattr(infer_config, \"tokenizer\", TOKENIZER)\n",
    "setattr(infer_config, \"soynlp_scores\", \"/media/scatter/scatterdisk/tokenizer/soynlp_scores.sol.100M.txt\")\n",
    "infer_preprocessor = DynamicPreprocessor(infer_config)\n",
    "infer_preprocessor.build_preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained embedding loaded. Number of OOV : 21 / 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angrypark/angryenv/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /media/scatter/scatterdisk/reply_matching_model/runs/soynlp_ns4_lr1e-3/best_loss/best_loss.ckpt\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "tf_config = tf.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "\n",
    "with graph.as_default():\n",
    "    Model = get_model(model_config.model)\n",
    "    infer_model = Model(preprocessor, model_config)\n",
    "    infer_sess = tf.Session(config=tf_config, graph=graph)\n",
    "    infer_sess.run(tf.global_variables_initializer())\n",
    "    infer_sess.run(tf.local_variables_initializer())\n",
    "\n",
    "infer_model.load(infer_sess, model_dir=best_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_model.save(infer_sess,\n",
    "                 os.path.join(model_config.checkpoint_dir, \"model.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(preprocessor):\n",
    "    base_dir = \"/home/angrypark/reply_matching_model/data/\"\n",
    "    with open(os.path.join(base_dir, \"test_queries.txt\"), \"r\") as f:\n",
    "        test_queries = [line.strip() for line in f]\n",
    "    with open(os.path.join(base_dir, \"test_replies.txt\"), \"r\") as f:\n",
    "        replies_set = [line.strip().split(\"\\t\") for line in f]\n",
    "    with open(os.path.join(base_dir, \"test_labels.txt\"), \"r\") as f:\n",
    "        test_labels = [[int(y) for y in line.strip().split(\"\\t\")] for line in f]\n",
    "\n",
    "    test_queries, test_queries_lengths = zip(*[preprocessor.preprocess(query)\n",
    "                                                     for query in test_queries])\n",
    "    test_replies = list()\n",
    "    test_replies_lengths = list()\n",
    "    for replies in replies_set:\n",
    "        r, l = zip(*[preprocessor.preprocess(reply) for reply in replies])\n",
    "        test_replies.append(r)\n",
    "        test_replies_lengths.append(l)\n",
    "    return test_queries, test_replies, test_queries_lengths, test_replies_lengths, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, sess, preprocessor):\n",
    "    test_queries, test_replies, test_queries_lengths, \\\n",
    "    test_replies_lengths, test_labels = load_test_data(preprocessor)\n",
    "\n",
    "    # flatten\n",
    "    row, col, _ = np.shape(test_replies)\n",
    "    test_queries_expanded = [[q]*col for q in test_queries]\n",
    "    test_queries_expanded = [y for x in test_queries_expanded for y in x]\n",
    "    test_queries_lengths_expanded = [[l]*col for l in test_queries_lengths]\n",
    "    test_queries_lengths_expanded = [y for x in test_queries_lengths_expanded for y in x]\n",
    "    test_replies = [y for x in test_replies for y in x]\n",
    "    test_replies_lengths = [y for x in test_replies_lengths for y in x]\n",
    "\n",
    "    feed_dict = {model.input_queries: test_queries_expanded,\n",
    "                 model.input_replies: test_replies,\n",
    "                 model.queries_lengths: test_queries_lengths_expanded,\n",
    "                 model.replies_lengths: test_replies_lengths, \n",
    "                 model.dropout_keep_prob: 1}\n",
    "    probs = model.infer(sess, feed_dict=feed_dict)\n",
    "    probs = np.reshape(probs, [row, col])\n",
    "    return test_labels, probs.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_prob = test(infer_model, infer_sess, preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, f1_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(y_true, y_prob, k=5):\n",
    "    def get_rank(y_true, y_prob):\n",
    "        rs = list()\n",
    "        for y_t, y_p in zip(y_true, y_prob):\n",
    "            r = sorted([(t, p) for t, p in zip(y_t, y_p)], key=lambda x: x[1], reverse=True)\n",
    "            r = [t for t, p in r]\n",
    "            rs.append(r)\n",
    "        return rs\n",
    "\n",
    "    def get_precision_at_k(rs, k):\n",
    "        rs = [(np.asarray(r)[:k] != 0) for r in rs]\n",
    "        return np.mean([np.mean(r) for r in rs])\n",
    "    \n",
    "    def mean_reciprocal_rank(rs):\n",
    "        rs = (np.asarray(r).nonzero()[0] for r in rs)\n",
    "        return np.mean([1. / (r[0] + 1) if r.size else 0. for r in rs])\n",
    "    \n",
    "    def dcg_at_k(r, k):\n",
    "        r = np.asfarray(r)[:k]\n",
    "        return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "    \n",
    "    def ndcg_at_k(r, k):\n",
    "        dcg_max = dcg_at_k(sorted(r, reverse=True), k)\n",
    "        if not dcg_max:\n",
    "            return 0.\n",
    "        return dcg_at_k(r, k) / dcg_max\n",
    "    \n",
    "    def mean_ndcg_at_k(rs, k):\n",
    "        return np.mean([ndcg_at_k(r, k) for r in rs])\n",
    "    \n",
    "    def flatten(list_of_lists):\n",
    "        return [y for x in list_of_lists for y in x]\n",
    "    \n",
    "    def get_best_threshold(y_true, y_prob):\n",
    "        y_true_binary = [y!=0 for y in flatten(y_true)]\n",
    "        precision, recall, thresholds = precision_recall_curve(y_true_binary, flatten(y_prob))\n",
    "        best_f_measure = 0\n",
    "        best_threshold = 0\n",
    "        for p, r, t in zip(precision, recall, thresholds):\n",
    "            if (p+r) == 0:\n",
    "                continue\n",
    "            f_measure = 2*p*r/(p+r)\n",
    "            if f_measure > best_f_measure:\n",
    "                best_f_measure = f_measure\n",
    "                best_threshold = t\n",
    "        return np.round(best_threshold, 2)\n",
    "    \n",
    "    def get_f1_score(y_true, y_prob, threshold):\n",
    "        return f1_score([y!=0 for y in flatten(y_true)], [int(y>=threshold) for y in flatten(y_prob)])\n",
    "    \n",
    "    rs = get_rank(y_true, y_prob)\n",
    "    threshold = get_best_threshold(y_true, y_prob)\n",
    "    f_measure = get_f1_score(y_true, y_prob, threshold)\n",
    "    \n",
    "    return {\"precision_at_{}\".format(k): get_precision_at_k(rs, k), \n",
    "            \"mrr\": mean_reciprocal_rank(rs), \n",
    "            \"ndcg\": mean_ndcg_at_k(rs, 10), \n",
    "            \"threshold\": threshold, \n",
    "            \"f1_score\": f_measure}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_epoch': '1',\n",
       " 'best_step': '80000',\n",
       " 'f1_score': 0.4714887102946805,\n",
       " 'model': 'DualEncoderLSTM',\n",
       " 'mrr': 0.6511217193147887,\n",
       " 'name': 'soynlp_ns1_lr1e-3',\n",
       " 'ndcg': 0.6870581862944968,\n",
       " 'negative_sampling': 'random',\n",
       " 'num_negative_samples': 1,\n",
       " 'precision_at_5': 0.3465346534653465,\n",
       " 'threshold': 0.01}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = {\"name\": model_config.name, \n",
    "          \"model\": model_config.model, \n",
    "          \"negative_sampling\": model_config.negative_sampling, \n",
    "          \"num_negative_samples\": model_config.num_negative_samples, \n",
    "          \"best_epoch\": model_config.best_epoch,\n",
    "          \"best_step\": model_config.best_step}\n",
    "result.update(evaluate_metrics(y_true, y_prob))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/angrypark/reply_matching_model/data/reply_set.txt\", \"r\") as f:\n",
    "    reply_set = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"나 오늘 술약속 있다\"\n",
    "\n",
    "indexed_query, query_length = infer_preprocessor.preprocess(query)\n",
    "indexed_replies, replies_lengths = zip(*[infer_preprocessor.preprocess(reply) for reply in reply_set])\n",
    "\n",
    "feed_dict = {infer_model.input_queries: [indexed_query]*len(reply_set),\n",
    "             infer_model.input_replies: indexed_replies,\n",
    "             infer_model.queries_lengths: [query_length]*len(reply_set),\n",
    "             infer_model.replies_lengths: replies_lengths,\n",
    "             infer_model.dropout_keep_prob: 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('약속 취소됐어요?', 0.99697006), ('저랑 약속해요', 0.99523675), ('전 술 잘 안먹어요', 0.99372214), ('오늘 같은 날은 소주를 마시고 싶네요', 0.9936784), ('술 잘먹는 편이에요?', 0.9933994)]\n"
     ]
    }
   ],
   "source": [
    "probs = infer_model.infer(infer_sess, feed_dict=feed_dict)\n",
    "probs = [p[0] for p in probs]\n",
    "\n",
    "print(sorted([(reply, prob) for reply, prob in zip(reply_set, probs)], key=lambda x: x[1], reverse=True)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "base_dir = \"/media/scatter/scatterdisk/reply_matching_model/runs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_2                        : 3080000\n",
      "start                          : 35000 \n",
      "sentpiece50K_ns4_lr1e-3        : 360000\n",
      "start_3                        : 3500000\n",
      "soynlp_ns4_lr1e-3              : 180000\n",
      "soynlp_ns1_lr1e-3              : 80000 \n",
      "sentpiece50K_ns1_lr1e-3        : 360000\n"
     ]
    }
   ],
   "source": [
    "for name in os.listdir(base_dir):\n",
    "    config_dir = os.path.join(base_dir, name, \"config.json\")\n",
    "    d = json.load(open(config_dir, \"r\"))\n",
    "    print(\"{:30s} : {:6}\".format(name, d[\"best_step\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 512,\n",
       " 'best_epoch': '5',\n",
       " 'best_loss': '0.36251',\n",
       " 'best_step': '3080000',\n",
       " 'checkpoint_dir': '/media/scatter/scatterdisk/reply_matching_model/runs/start_2/',\n",
       " 'config': '',\n",
       " 'dropout_keep_prob': 0.9,\n",
       " 'embed_dim': 256,\n",
       " 'evaluate_every': 20000,\n",
       " 'gpu': 'a',\n",
       " 'learning_rate': 0.0001,\n",
       " 'lstm_dim': 512,\n",
       " 'max_length': 20,\n",
       " 'max_to_keep': 5,\n",
       " 'min_length': 1,\n",
       " 'mode': 'train',\n",
       " 'model': 'DualEncoderLSTM',\n",
       " 'name': 'start_2',\n",
       " 'negative_sampling': 'random',\n",
       " 'normalizer': 'DummyNormalizer',\n",
       " 'num_epochs': 20,\n",
       " 'num_negative_samples': 1,\n",
       " 'pretrained_embed_dir': '/media/scatter/scatterdisk/pretrained_embedding/fasttext.sent_piece_50K.256D',\n",
       " 'save_every': 10000,\n",
       " 'sent_piece_model': '/media/scatter/scatterdisk/tokenizer/sent_piece.50K.model',\n",
       " 'shuffle': True,\n",
       " 'tokenizer': 'DummyTokenizer',\n",
       " 'train_dir': '/media/scatter/scatterdisk/reply_matching_model/sol.tokenized.sent_piece_50K/',\n",
       " 'val_dir': '/media/scatter/scatterdisk/reply_matching_model/sol.tokenized.sent_piece_50K/sol.validation.txt',\n",
       " 'vocab_list': '/media/scatter/scatterdisk/pretrained_embedding/vocab_list.sent_piece_50K.txt',\n",
       " 'vocab_size': 100000}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/media/scatter/scatterdisk/reply_matching_model/runs/soynlp_ns4_lr1e-3/config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "angryenv",
   "language": "python",
   "name": "angryenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
