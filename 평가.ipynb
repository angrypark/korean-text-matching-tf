{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이름만 바꿔서 실행하면 됩니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = FastText.load(\"/media/scatter/scatterdisk/pretrained_embedding/fasttext.syllable.128D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "from data_loader import DataGenerator\n",
    "\n",
    "from trainer import MatchingModelTrainer\n",
    "from preprocessor import DynamicPreprocessor\n",
    "from utils.dirs import create_dirs\n",
    "from utils.logger import SummaryWriter\n",
    "from utils.config import load_config, save_config\n",
    "from models.base import get_model\n",
    "from utils.utils import JamoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"delstm_1024_nsrandom4_lr1e-3\"\n",
    "TOKENIZER = \"SentencePieceTokenizer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/media/scatter/scatterdisk/reply_matching_model/runs/{}/\".format(NAME)\n",
    "config_dir = base_dir + \"config.json\"\n",
    "# best_model_dir = base_dir + \"best_loss/best_loss.ckpt\"\n",
    "best_model_dir = base_dir + \"model.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = load_config(config_dir)\n",
    "preprocessor = DynamicPreprocessor(model_config)\n",
    "preprocessor.build_preprocessor()\n",
    "\n",
    "infer_config = load_config(config_dir)\n",
    "setattr(infer_config, \"tokenizer\", TOKENIZER)\n",
    "setattr(infer_config, \"soynlp_scores\", \"/media/scatter/scatterdisk/tokenizer/soynlp_scores.sol.100M.txt\")\n",
    "infer_preprocessor = DynamicPreprocessor(infer_config)\n",
    "infer_preprocessor.build_preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config.add_echo = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained embedding loaded. Number of OOV : 5272 / 90000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angrypark/angryenv/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /media/scatter/scatterdisk/reply_matching_model/runs/delstm_1024_nsrandom4_lr1e-3/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "tf_config = tf.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "\n",
    "with graph.as_default():\n",
    "    Model = get_model(model_config.model)\n",
    "    data = DataGenerator(preprocessor, model_config)\n",
    "    infer_model = Model(data, model_config)\n",
    "    infer_sess = tf.Session(config=tf_config, graph=graph)\n",
    "    infer_sess.run(tf.global_variables_initializer())\n",
    "    infer_sess.run(tf.local_variables_initializer())\n",
    "\n",
    "infer_model.load(infer_sess, model_dir=best_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(preprocessor):\n",
    "    base_dir = \"/home/angrypark/reply_matching_model/data/\"\n",
    "    with open(os.path.join(base_dir, \"test_queries.txt\"), \"r\") as f:\n",
    "        test_queries = [line.strip() for line in f]\n",
    "    with open(os.path.join(base_dir, \"test_replies.txt\"), \"r\") as f:\n",
    "        replies_set = [line.strip().split(\"\\t\") for line in f]\n",
    "    with open(os.path.join(base_dir, \"test_labels.txt\"), \"r\") as f:\n",
    "        test_labels = [[int(y) for y in line.strip().split(\"\\t\")] for line in f]\n",
    "\n",
    "    test_queries, test_queries_lengths = zip(*[preprocessor.preprocess(query)\n",
    "                                                     for query in test_queries])\n",
    "    test_replies = list()\n",
    "    test_replies_lengths = list()\n",
    "    for replies in replies_set:\n",
    "        r, l = zip(*[preprocessor.preprocess(reply) for reply in replies])\n",
    "        test_replies.append(r)\n",
    "        test_replies_lengths.append(l)\n",
    "    return test_queries, test_replies, test_queries_lengths, test_replies_lengths, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, sess, preprocessor):\n",
    "    test_queries, test_replies, test_queries_lengths, \\\n",
    "    test_replies_lengths, test_labels = load_test_data(preprocessor)\n",
    "\n",
    "    # flatten\n",
    "    row, col, _ = np.shape(test_replies)\n",
    "    test_queries_expanded = [[q]*col for q in test_queries]\n",
    "    test_queries_expanded = [y for x in test_queries_expanded for y in x]\n",
    "    test_queries_lengths_expanded = [[l]*col for l in test_queries_lengths]\n",
    "    test_queries_lengths_expanded = [y for x in test_queries_lengths_expanded for y in x]\n",
    "    test_replies = [y for x in test_replies for y in x]\n",
    "    test_replies_lengths = [y for x in test_replies_lengths for y in x]\n",
    "\n",
    "    feed_dict = {model.input_queries: test_queries_expanded,\n",
    "                 model.input_replies: test_replies,\n",
    "                 model.queries_lengths: test_queries_lengths_expanded,\n",
    "                 model.replies_lengths: test_replies_lengths, \n",
    "                 model.dropout_keep_prob: 1}\n",
    "    probs = model.infer(sess, feed_dict=feed_dict)\n",
    "    probs = np.reshape(probs, [row, col])\n",
    "    return test_labels, probs.tolist()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def hackathon(model, sess, preprocessor):\n",
    "    with open(\"../paraphrase_detection/data/test_queries.txt\", \"r\") as f:\n",
    "        test_queries = [line.strip().split(\"\\t\")[1] for line in f]\n",
    "    with open(\"../paraphrase_detection/data/test_replies.txt\", \"r\") as f:\n",
    "        test_replies = [line.strip().split(\"\\t\")[1] for line in f]\n",
    "    \n",
    "    preprocessed_replies, replies_lengths = zip(*[preprocessor.preprocess(sentence) for sentence in test_replies])\n",
    "    length = len(preprocessed_replies)\n",
    "    \n",
    "    for i, query in enumerate(test_queries):\n",
    "        preprocessed_query, _ = preprocessor.preprocess(query)\n",
    "        feed_dict = {model.input_queries: [preprocessed_query]*length,\n",
    "                 model.input_replies: preprocessed_replies,\n",
    "                 model.queries_lengths: [len(query)]*length,\n",
    "                 model.replies_lengths: replies_lengths,\n",
    "                 model.dropout_keep_prob: 1}\n",
    "        probs = model.infer(sess, feed_dict=feed_dict)\n",
    "        probs = [(i, prob) for i, prob in enumerate(probs)]\n",
    "        probs = [(i, reply, prob) for reply, (i, prob) in zip(test_replies, probs)]\n",
    "        select = [line[1] for line in sorted(probs, key=lambda x: x[2], reverse=True)[:3]]\n",
    "        print(i, query, select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_prob = test(infer_model, infer_sess, infer_preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, f1_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(y_true, y_prob, k=5):\n",
    "    def get_rank(y_true, y_prob):\n",
    "        rs = list()\n",
    "        for y_t, y_p in zip(y_true, y_prob):\n",
    "            r = sorted([(t, p) for t, p in zip(y_t, y_p)], key=lambda x: x[1], reverse=True)\n",
    "            r = [t for t, p in r]\n",
    "            rs.append(r)\n",
    "        return rs\n",
    "\n",
    "    def get_precision_at_k(rs, k):\n",
    "        rs = [(np.asarray(r)[:k] != 0) for r in rs]\n",
    "        return np.mean([np.mean(r) for r in rs])\n",
    "    \n",
    "    def mean_reciprocal_rank(rs):\n",
    "        rs = (np.asarray(r).nonzero()[0] for r in rs)\n",
    "        return np.mean([1. / (r[0] + 1) if r.size else 0. for r in rs])\n",
    "    \n",
    "    def dcg_at_k(r, k):\n",
    "        r = np.asfarray(r)[:k]\n",
    "        return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "    \n",
    "    def ndcg_at_k(r, k):\n",
    "        dcg_max = dcg_at_k(sorted(r, reverse=True), k)\n",
    "        if not dcg_max:\n",
    "            return 0.\n",
    "        return dcg_at_k(r, k) / dcg_max\n",
    "    \n",
    "    def mean_ndcg_at_k(rs, k):\n",
    "        return np.mean([ndcg_at_k(r, k) for r in rs])\n",
    "    \n",
    "    def flatten(list_of_lists):\n",
    "        return [y for x in list_of_lists for y in x]\n",
    "    \n",
    "    def get_best_threshold(y_true, y_prob):\n",
    "        y_true_binary = [y!=0 for y in flatten(y_true)]\n",
    "        precision, recall, thresholds = precision_recall_curve(y_true_binary, flatten(y_prob))\n",
    "        best_f_measure = 0\n",
    "        best_threshold = 0\n",
    "        for p, r, t in zip(precision, recall, thresholds):\n",
    "            if (p+r) == 0:\n",
    "                continue\n",
    "            f_measure = 2*p*r/(p+r)\n",
    "            if f_measure > best_f_measure:\n",
    "                best_f_measure = f_measure\n",
    "                best_threshold = t\n",
    "        return np.round(best_threshold, 2)\n",
    "    \n",
    "    def get_f1_score(y_true, y_prob, threshold):\n",
    "        return f1_score([y!=0 for y in flatten(y_true)], [int(y>=threshold) for y in flatten(y_prob)])\n",
    "    \n",
    "    rs = get_rank(y_true, y_prob)\n",
    "    threshold = get_best_threshold(y_true, y_prob)\n",
    "    f_measure = get_f1_score(y_true, y_prob, threshold)\n",
    "    \n",
    "    return {\"precision_at_{}\".format(k): get_precision_at_k(rs, k), \n",
    "            \"mrr\": mean_reciprocal_rank(rs), \n",
    "            \"ndcg\": mean_ndcg_at_k(rs, 10), \n",
    "            \"threshold\": threshold, \n",
    "            \"f1_score\": f_measure}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epoch': 4,\n",
       " 'f1_score': 0.5118012422360249,\n",
       " 'model': 'DualEncoderLSTM',\n",
       " 'mrr': 0.7244558384409869,\n",
       " 'name': 'delstm_1024_nsrandom4_lr1e-3',\n",
       " 'ndcg': 0.7488139150267936,\n",
       " 'negative_sampling': 'random',\n",
       " 'num_negative_samples': 4,\n",
       " 'precision_at_5': 0.42772277227722777,\n",
       " 'step': 3759999,\n",
       " 'threshold': 0.56}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = {\"name\": model_config.name, \n",
    "          \"model\": model_config.model, \n",
    "          \"negative_sampling\": model_config.negative_sampling, \n",
    "          \"num_negative_samples\": model_config.num_negative_samples, \n",
    "          \"epoch\": infer_model.cur_epoch_tensor.eval(infer_sess),\n",
    "          \"step\": infer_model.global_step_tensor.eval(infer_sess)}\n",
    "result.update(evaluate_metrics(y_true, y_prob))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 2]), array([2, 3])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(np.array([[1,2], [2,3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 35s, sys: 28.8 s, total: 3min 4s\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(\"/home/angrypark/paraphrase_detection/data/small/train.txt\", \"r\") as f:\n",
    "    train_set = {\"sentence_A\": list(), \n",
    "                 \"sentence_B\": list(), \n",
    "                 \"ab_prob\": list(), \n",
    "                 \"ba_prob\": list(), \n",
    "                 \"semantic_sim\": list(), \n",
    "                 \"label\": list()}\n",
    "    batch = list()\n",
    "    for line in f:\n",
    "        batch.append(line)\n",
    "        if len(batch) % 512 == 0:\n",
    "            A, B, labels = zip(*[line.strip().split(\"\\t\") for line in batch])\n",
    "            indexed_A, A_lengths = zip(*[infer_preprocessor.preprocess(a) for a in A])\n",
    "            indexed_B, B_lengths = zip(*[infer_preprocessor.preprocess(b) for b in B])\n",
    "            feed_dict = {infer_model.input_queries: indexed_A,\n",
    "                 infer_model.input_replies: indexed_B,\n",
    "                 infer_model.queries_lengths: A_lengths,\n",
    "                 infer_model.replies_lengths: B_lengths,\n",
    "                 infer_model.dropout_keep_prob: 1, \n",
    "                 }\n",
    "            A_sentence_vectors, AB_probs = infer_sess.run([infer_model.encoding_queries, \n",
    "                                                           infer_model.positive_probs], \n",
    "                                                          feed_dict=feed_dict)\n",
    "            \n",
    "            feed_dict = {infer_model.input_queries: indexed_B,\n",
    "                 infer_model.input_replies: indexed_A,\n",
    "                 infer_model.queries_lengths: B_lengths,\n",
    "                 infer_model.replies_lengths: A_lengths,\n",
    "                 infer_model.dropout_keep_prob: 1,\n",
    "                 }\n",
    "            B_sentence_vectors, BA_probs = infer_sess.run([infer_model.encoding_queries, \n",
    "                                                           infer_model.positive_probs], \n",
    "                                                          feed_dict=feed_dict)\n",
    "            semantic_sim = [cosine_similarity([a_vector], [b_vector])[0][0] for a_vector, b_vector in zip(list(A_sentence_vectors), list(B_sentence_vectors))]\n",
    "            \n",
    "            train_set[\"sentence_A\"] += A\n",
    "            train_set[\"sentence_B\"] += B\n",
    "            train_set[\"ab_prob\"] += [p[0] for p in list(AB_probs)]\n",
    "            train_set[\"ba_prob\"] += [p[0] for p in list(BA_probs)]\n",
    "            train_set[\"semantic_sim\"] += semantic_sim\n",
    "            train_set[\"label\"] += labels\n",
    "            \n",
    "            batch = list()\n",
    "    A, B, labels = zip(*[line.strip().split(\"\\t\") for line in batch])\n",
    "    indexed_A, A_lengths = zip(*[infer_preprocessor.preprocess(a) for a in A])\n",
    "    indexed_B, B_lengths = zip(*[infer_preprocessor.preprocess(b) for b in B])\n",
    "    feed_dict = {infer_model.input_queries: indexed_A,\n",
    "         infer_model.input_replies: indexed_B,\n",
    "         infer_model.queries_lengths: A_lengths,\n",
    "         infer_model.replies_lengths: B_lengths,\n",
    "         infer_model.dropout_keep_prob: 1, \n",
    "         }\n",
    "    A_sentence_vectors, AB_probs = infer_sess.run([infer_model.encoding_queries, \n",
    "                                                   infer_model.positive_probs], \n",
    "                                                  feed_dict=feed_dict)\n",
    "\n",
    "    feed_dict = {infer_model.input_queries: indexed_B,\n",
    "         infer_model.input_replies: indexed_A,\n",
    "         infer_model.queries_lengths: B_lengths,\n",
    "         infer_model.replies_lengths: A_lengths,\n",
    "         infer_model.dropout_keep_prob: 1,\n",
    "         }\n",
    "    B_sentence_vectors, BA_probs = infer_sess.run([infer_model.encoding_queries, \n",
    "                                                   infer_model.positive_probs], \n",
    "                                                  feed_dict=feed_dict)\n",
    "    semantic_sim = [cosine_similarity([a_vector], [b_vector])[0][0] for a_vector, b_vector in zip(list(A_sentence_vectors), list(B_sentence_vectors))]\n",
    "\n",
    "    train_set[\"sentence_A\"] += A\n",
    "    train_set[\"sentence_B\"] += B\n",
    "    train_set[\"ab_prob\"] += [p[0] for p in list(AB_probs)]\n",
    "    train_set[\"ba_prob\"] += [p[0] for p in list(BA_probs)]\n",
    "    train_set[\"semantic_sim\"] += semantic_sim\n",
    "    train_set[\"label\"] += labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(train_set, open(\"../paraphrase_detection/data/train_set.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/angrypark/reply_matching_model/data/reply_set.txt\", \"r\") as f:\n",
    "    reply_set = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"나 오늘 술약속 있다\"\n",
    "\n",
    "indexed_query, query_length = infer_preprocessor.preprocess(query)\n",
    "indexed_replies, replies_lengths = zip(*[infer_preprocessor.preprocess(reply) for reply in reply_set])\n",
    "\n",
    "feed_dict = {infer_model.input_queries: [indexed_query]*len(reply_set),\n",
    "             infer_model.input_replies: indexed_replies,\n",
    "             infer_model.queries_lengths: [query_length]*len(reply_set),\n",
    "             infer_model.replies_lengths: replies_lengths,\n",
    "             infer_model.dropout_keep_prob: 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('약속 취소됐어요?', 0.99920183), ('저랑 약속해요', 0.994364), ('저녁에 술 한 잔 할까요?', 0.97491246), ('오늘 회식이에요?', 0.9738028), ('요즘 회식은 주로 목요일에 하지 않아요?', 0.9341068)]\n"
     ]
    }
   ],
   "source": [
    "probs = infer_model.infer(infer_sess, feed_dict=feed_dict)\n",
    "probs = [p[0] for p in probs]\n",
    "\n",
    "print(sorted([(reply, prob) for reply, prob in zip(reply_set, probs)], key=lambda x: x[1], reverse=True)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "base_dir = \"/media/scatter/scatterdisk/reply_matching_model/runs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentpiece100K_ns4_lr1e-3       : 1480000\n",
      "delstm_nshard4_lr3e-4          : 40000 \n",
      "start_2                        : 3080000\n",
      "delstm_1024_nsrandom4_lr1e-3   : 1780000\n",
      "sentpiece50K_ns4_lr1e-3        : 480000\n",
      "delstm_nsrandom4echo_lr1e-3    : 1600000\n",
      "detcn_nsrandom4_lr1e-3         : 1800000\n",
      "delstm_nsrandom9_lr1e-3        : 20000 \n",
      "sentpiece100K_ns1_lr1e-3       : 1480000\n",
      "debug_embedding                : 129   \n",
      "start_3                        : 3500000\n",
      "soynlp_ns4_lr1e-3              : 400000\n",
      "soynlp_ns1_lr1e-3              : 340000\n",
      "sentpiece50K_ns1_lr1e-3        : 480000\n"
     ]
    }
   ],
   "source": [
    "for name in os.listdir(base_dir):\n",
    "    config_dir = os.path.join(base_dir, name, \"config.json\")\n",
    "    try:\n",
    "        d = json.load(open(config_dir, \"r\"))\n",
    "        print(\"{:30s} : {:6}\".format(name, d[\"best_step\"]))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 512,\n",
       " 'best_epoch': '5',\n",
       " 'best_loss': '0.36251',\n",
       " 'best_step': '3080000',\n",
       " 'checkpoint_dir': '/media/scatter/scatterdisk/reply_matching_model/runs/start_2/',\n",
       " 'config': '',\n",
       " 'dropout_keep_prob': 0.9,\n",
       " 'embed_dim': 256,\n",
       " 'evaluate_every': 20000,\n",
       " 'gpu': 'a',\n",
       " 'learning_rate': 0.0001,\n",
       " 'lstm_dim': 512,\n",
       " 'max_length': 20,\n",
       " 'max_to_keep': 5,\n",
       " 'min_length': 1,\n",
       " 'mode': 'train',\n",
       " 'model': 'DualEncoderLSTM',\n",
       " 'name': 'start_2',\n",
       " 'negative_sampling': 'random',\n",
       " 'normalizer': 'DummyNormalizer',\n",
       " 'num_epochs': 20,\n",
       " 'num_negative_samples': 1,\n",
       " 'pretrained_embed_dir': '/media/scatter/scatterdisk/pretrained_embedding/fasttext.sent_piece_50K.256D',\n",
       " 'save_every': 10000,\n",
       " 'sent_piece_model': '/media/scatter/scatterdisk/tokenizer/sent_piece.50K.model',\n",
       " 'shuffle': True,\n",
       " 'tokenizer': 'DummyTokenizer',\n",
       " 'train_dir': '/media/scatter/scatterdisk/reply_matching_model/sol.tokenized.sent_piece_50K/',\n",
       " 'val_dir': '/media/scatter/scatterdisk/reply_matching_model/sol.tokenized.sent_piece_50K/sol.validation.txt',\n",
       " 'vocab_list': '/media/scatter/scatterdisk/pretrained_embedding/vocab_list.sent_piece_50K.txt',\n",
       " 'vocab_size': 100000}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/media/scatter/scatterdisk/reply_matching_model/runs/soynlp_ns4_lr1e-3/config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "angryenv",
   "language": "python",
   "name": "angryenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
